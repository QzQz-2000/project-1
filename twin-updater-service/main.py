import asyncio
import aiohttp
import json
import logging
import os  # æ·»åŠ ç¼ºå°‘çš„ import
import redis.asyncio as redis
from datetime import datetime, timezone
from typing import Dict, List, Optional
from dataclasses import dataclass
from motor.motor_asyncio import AsyncIOMotorClient

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class BatchUpdateItem: 
    twin_id: str
    environment_id: str
    properties: Dict
    timestamp: datetime

class Metrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    def __init__(self):
        self.messages_consumed = 0
        self.twins_updated = 0
        self.update_failures = 0
        self.notifications_sent = 0
        self.start_time = datetime.now()
    
    def log_status(self):
        """è¾“å‡ºçŠ¶æ€ç»Ÿè®¡"""
        uptime = datetime.now() - self.start_time
        logger.info(f"çŠ¶æ€ç»Ÿè®¡ - è¿è¡Œæ—¶é—´: {uptime}, æ¶ˆæ¯æ¶ˆè´¹: {self.messages_consumed}, "
                   f"Twinæ›´æ–°: {self.twins_updated}, å¤±è´¥æ¬¡æ•°: {self.update_failures}, "
                   f"é€šçŸ¥å‘é€: {self.notifications_sent}")

class TelemetryMessage:
    """é¥æµ‹æ¶ˆæ¯æ¨¡å‹"""
    def __init__(self, **kwargs):
        self.environment_id = kwargs.get('environment_id')
        self.device_id = kwargs.get('device_id')
        self.payload = kwargs.get('payload', {})
        self.timestamp = kwargs.get('timestamp')
        if isinstance(self.timestamp, str):
            self.timestamp = datetime.fromisoformat(self.timestamp.replace('Z', '+00:00'))


class Settings:
    """é…ç½®ç®¡ç†"""
    # APIé…ç½® - æ™ºèƒ½é»˜è®¤å€¼
    API_BASE_URL = os.getenv("API_BASE_URL", "http://host.docker.internal:8000")
    API_BATCH_SIZE = int(os.getenv("API_BATCH_SIZE", "50"))
    API_BATCH_TIMEOUT = float(os.getenv("API_BATCH_TIMEOUT", "5.0"))  # ç§’
    
    # Kafkaé…ç½®
    KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
    KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "telemetry")
    KAFKA_GROUP_ID = os.getenv("KAFKA_GROUP_ID", "twin-updater")
    KAFKA_AUTO_OFFSET_RESET = os.getenv("KAFKA_AUTO_OFFSET_RESET", "latest")
    
    # MongoDBé…ç½® - ç¡®ä¿å®¹å™¨ç¯å¢ƒä¸‹çš„æ­£ç¡®é»˜è®¤å€¼
    MONGO_DB_URL = os.getenv("MONGO_DB_URL", "mongodb://my-mongo:27017")
    MONGO_DB_NAME = os.getenv("MONGO_DB_NAME", "digital_twin_db")
    
    # Redisé…ç½® - æ™ºèƒ½é»˜è®¤å€¼
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6380")
    
    # WebSocketé€šçŸ¥é…ç½®
    WEBSOCKET_NOTIFICATION_ENABLED = os.getenv("WEBSOCKET_NOTIFICATION_ENABLED", "true").lower() == "true"

settings = Settings()

class WebSocketNotificationService:
    """WebSocketé€šçŸ¥æœåŠ¡"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.enabled = settings.WEBSOCKET_NOTIFICATION_ENABLED
    
    async def send_twin_properties_updated_notification(self, environment_id: str, twin_id: str, 
                                                       updated_properties: Dict, timestamp: datetime):
        """å‘é€Twinå±æ€§æ›´æ–°é€šçŸ¥"""
        if not self.enabled:
            return
        try:
            # ç»Ÿä¸€æ—¶é—´æˆ³æ ¼å¼åŒ–
            if timestamp.tzinfo is not None:
                utc_timestamp = timestamp.astimezone(timezone.utc)
            else:
                utc_timestamp = timestamp.replace(tzinfo=timezone.utc)
            
            notification = {
                "type": "twin_properties_updated",
                "environment_id": environment_id,
                "twin_id": twin_id,
                "updated_properties": updated_properties,
                "timestamp": utc_timestamp.strftime('%Y-%m-%dT%H:%M:%SZ'),
                "event_time": datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            # å‘å¸ƒåˆ°Redisé¢‘é“ï¼ŒWebSocketæœåŠ¡å™¨ä¼šç›‘å¬è¿™ä¸ªé¢‘é“
            channel = f"ws_notifications:{environment_id}"
            await self.redis_client.publish(channel, json.dumps(notification))
            
            logger.debug(f"å‘é€Twinå±æ€§æ›´æ–°é€šçŸ¥: {twin_id} -> {environment_id}")
            
        except Exception as e:
            logger.error(f"å‘é€Twinå±æ€§æ›´æ–°é€šçŸ¥å¤±è´¥: {e}")
    
    async def send_system_status_notification(self, environment_id: str, status: Dict):
        """å‘é€ç³»ç»ŸçŠ¶æ€é€šçŸ¥"""
        if not self.enabled:
            return
        
        try:
            notification = {
                "type": "system_status",
                "environment_id": environment_id,
                "status": status,
                "event_time": datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            channel = f"ws_notifications:{environment_id}"
            await self.redis_client.publish(channel, json.dumps(notification))
            
        except Exception as e:
            logger.error(f"å‘é€ç³»ç»ŸçŠ¶æ€é€šçŸ¥å¤±è´¥: {e}")

class OptimizedTwinUpdaterService:
    """ä¼˜åŒ–çš„Twinæ›´æ–°æœåŠ¡ - æ”¯æŒå®æ—¶é€šçŸ¥"""
    
    def __init__(self):
        self.running = True
        self.consumer = None
        self.db_client = None
        self.db = None
        self.http_session = None
        self.redis_client = None
        self.notification_service = None
        self._tasks = []
        
        # æ‰¹é‡æ›´æ–°é˜Ÿåˆ— - æŒ‰environmentåˆ†ç»„
        self.update_queues = {}  # environment_id -> Queue
        self.metrics = Metrics()
        
        # APIé…ç½®
        self.api_base_url = settings.API_BASE_URL
        self.api_timeout = aiohttp.ClientTimeout(total=30)
        self.max_batch_size = settings.API_BATCH_SIZE
        self.batch_timeout = settings.API_BATCH_TIMEOUT

    async def connect_to_databases(self):
        """åˆå§‹åŒ–è¿æ¥"""
        try:
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            logger.info(f"=== æœåŠ¡é…ç½®ä¿¡æ¯ ===")
            logger.info(f"MongoDB URL: {settings.MONGO_DB_URL}")
            logger.info(f"MongoDB DB Name: {settings.MONGO_DB_NAME}")
            logger.info(f"Redis URL: {settings.REDIS_URL}")
            logger.info(f"API Base URL: {settings.API_BASE_URL}")
            logger.info(f"Kafka Servers: {settings.KAFKA_BOOTSTRAP_SERVERS}")
            logger.info(f"==================")
            
            # MongoDBè¿æ¥ï¼ˆç”¨äºæŸ¥è¯¢æ˜ å°„å…³ç³»ï¼‰
            logger.info(f"å°è¯•è¿æ¥MongoDB: {settings.MONGO_DB_URL}")
            self.db_client = AsyncIOMotorClient(settings.MONGO_DB_URL, serverSelectionTimeoutMS=10000)
            self.db = self.db_client[settings.MONGO_DB_NAME]
            await self.db.command('ping')
            logger.info("MongoDBè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"MongoDBè¿æ¥å¤±è´¥: {e}")
            raise
        
        try:
            # Redisè¿æ¥ï¼ˆç”¨äºWebSocketé€šçŸ¥ï¼‰
            logger.info(f"å°è¯•è¿æ¥Redis: {settings.REDIS_URL}")
            self.redis_client = redis.from_url(settings.REDIS_URL)
            await self.redis_client.ping()
            logger.info("Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"Redisè¿æ¥å¤±è´¥: {e} - WebSocketé€šçŸ¥å°†è¢«ç¦ç”¨")
            self.redis_client = None
        
        # åˆå§‹åŒ–é€šçŸ¥æœåŠ¡
        if self.redis_client:
            self.notification_service = WebSocketNotificationService(self.redis_client)
        
        # HTTP Session with connection pooling
        connector = aiohttp.TCPConnector(
            limit=100,  # æ€»è¿æ¥æ± å¤§å°
            limit_per_host=20,  # æ¯ä¸ªhostçš„è¿æ¥æ•°
            keepalive_timeout=60,
            enable_cleanup_closed=True
        )
        
        self.http_session = aiohttp.ClientSession(
            connector=connector,
            timeout=self.api_timeout,
            headers={'Content-Type': 'application/json'}
        )
        
        logger.info("æ•°æ®åº“å’ŒHTTPè¿æ¥åˆå§‹åŒ–å®Œæˆ")

    async def start(self):
        """å¯åŠ¨æœåŠ¡"""
        await self.connect_to_databases()
        
        try:
            from aiokafka import AIOKafkaConsumer
            logger.info(f"å°è¯•è¿æ¥Kafka: {settings.KAFKA_BOOTSTRAP_SERVERS}")
            
            self.consumer = AIOKafkaConsumer(
                settings.KAFKA_TOPIC,
                bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
                group_id=settings.KAFKA_GROUP_ID,
                value_deserializer=lambda m: json.loads(m.decode("utf-8")),
                auto_offset_reset=settings.KAFKA_AUTO_OFFSET_RESET,
                consumer_timeout_ms=1000,  # æ·»åŠ è¶…æ—¶ï¼Œé¿å…é˜»å¡
            )

            await self.consumer.start()
            logger.info("Kafkaè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"Kafkaè¿æ¥å¤±è´¥: {e}")
            raise
        
        self._tasks = [
            asyncio.create_task(self.consume_loop()),
            asyncio.create_task(self.monitor_loop()),
            asyncio.create_task(self.status_broadcaster_loop())  # çŠ¶æ€å¹¿æ’­å¾ªç¯
        ]
        
        logger.info("Twin Updater Serviceå¯åŠ¨æˆåŠŸ")
        await asyncio.gather(*self._tasks, return_exceptions=True)

    async def consume_loop(self):
        """æ¶ˆè´¹Kafkaæ¶ˆæ¯"""
        try:
            async for msg in self.consumer:
                if not self.running:
                    break
                    
                try:
                    telemetry = TelemetryMessage(**msg.value)
                    self.metrics.messages_consumed += 1
                    
                    # æŸ¥è¯¢è®¾å¤‡æ˜ å°„å…³ç³»
                    mapping = await self.db["device_twin_mappings"].find_one({
                        "environment_id": telemetry.environment_id,
                        "device_id": telemetry.device_id
                    })
                    
                    if not mapping:
                        logger.debug(f"è®¾å¤‡ {telemetry.device_id} æ— æ˜ å°„å…³ç³»")
                        continue
                    
                    # åˆ›å»ºæ›´æ–°é¡¹
                    update_item = BatchUpdateItem(
                        twin_id=mapping["twin_id"],
                        environment_id=telemetry.environment_id,
                        properties=telemetry.payload,
                        timestamp=telemetry.timestamp
                    )
                    
                    # æ·»åŠ åˆ°å¯¹åº”ç¯å¢ƒçš„æ‰¹é‡é˜Ÿåˆ—
                    await self._add_to_batch_queue(update_item)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")
                    self.metrics.update_failures += 1
        except Exception as e:
            logger.error(f"æ¶ˆè´¹å¾ªç¯å¼‚å¸¸: {e}")
            if self.running:
                raise

    async def _add_to_batch_queue(self, update_item: BatchUpdateItem):
        """æ·»åŠ åˆ°æ‰¹é‡æ›´æ–°é˜Ÿåˆ—"""
        env_id = update_item.environment_id
        
        # ç¡®ä¿ç¯å¢ƒé˜Ÿåˆ—å­˜åœ¨
        if env_id not in self.update_queues:
            self.update_queues[env_id] = asyncio.Queue(maxsize=10000)
            # ä¸ºè¿™ä¸ªç¯å¢ƒå¯åŠ¨æ‰¹é‡å¤„ç†ä»»åŠ¡
            task = asyncio.create_task(self._batch_update_loop(env_id))
            self._tasks.append(task)
        
        try:
            await self.update_queues[env_id].put(update_item)
        except asyncio.QueueFull:
            logger.warning(f"ç¯å¢ƒ {env_id} çš„æ›´æ–°é˜Ÿåˆ—å·²æ»¡")
            self.metrics.update_failures += 1

    async def _batch_update_loop(self, environment_id: str):
        """é’ˆå¯¹ç‰¹å®šç¯å¢ƒçš„æ‰¹é‡æ›´æ–°å¾ªç¯"""
        queue = self.update_queues[environment_id]
        batch = {}  # twin_id -> properties
        last_flush_time = asyncio.get_event_loop().time()
        
        while self.running or not queue.empty():
            try:
                # å°è¯•è·å–æ›´æ–°é¡¹
                try:
                    update_item = await asyncio.wait_for(
                        queue.get(), 
                        timeout=self.batch_timeout
                    )
                    
                    # åˆå¹¶åŒä¸€twinçš„å¤šæ¬¡æ›´æ–°
                    twin_id = update_item.twin_id
                    if twin_id not in batch:
                        batch[twin_id] = {
                            "properties": {},
                            "timestamp": update_item.timestamp
                        }
                    
                    # æ›´æ–°å±æ€§ï¼ˆåæ¥çš„è¦†ç›–å‰é¢çš„ï¼‰
                    batch[twin_id]["properties"].update(update_item.properties)
                    # ä½¿ç”¨æœ€æ–°çš„æ—¶é—´æˆ³
                    if update_item.timestamp > batch[twin_id]["timestamp"]:
                        batch[twin_id]["timestamp"] = update_item.timestamp
                    
                except asyncio.TimeoutError:
                    pass
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
                current_time = asyncio.get_event_loop().time()
                should_flush = (
                    len(batch) >= self.max_batch_size or
                    (current_time - last_flush_time >= self.batch_timeout and batch) or
                    (not self.running and batch)
                )
                
                if should_flush:
                    await self._execute_api_batch_update(environment_id, batch)
                    batch.clear()
                    last_flush_time = current_time
                    
            except Exception as e:
                logger.error(f"ç¯å¢ƒ {environment_id} æ‰¹é‡æ›´æ–°å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(1)

    async def _execute_api_batch_update(self, environment_id: str, batch: Dict):
        """æ‰§è¡ŒAPIæ‰¹é‡æ›´æ–°"""
        if not batch:
            return
        
        logger.info("ğŸ”§ ä½¿ç”¨ä¿®å¤åçš„æ—¶é—´æˆ³æ ¼å¼åŒ–ä»£ç !")  # æ˜æ˜¾çš„æ ‡è¯†
            
        # å‡†å¤‡æ‰¹é‡æ›´æ–°æ•°æ®
        batch_updates = []
        for twin_id, data in batch.items():
            # ç®€å•ä¸”å¯é çš„æ—¶é—´æˆ³æ ¼å¼åŒ–
            timestamp = data["timestamp"]
            # ç»Ÿä¸€è½¬æ¢ä¸ºUTCæ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºZç»“å°¾çš„ISOæ ¼å¼
            if timestamp.tzinfo is not None:
                utc_timestamp = timestamp.astimezone(timezone.utc)
            else:
                utc_timestamp = timestamp.replace(tzinfo=timezone.utc)
            
            timestamp_str = utc_timestamp.strftime('%Y-%m-%dT%H:%M:%SZ')
            
            batch_updates.append({
                "twin_id": twin_id,
                "properties": data["properties"],
                "telemetry_last_updated": timestamp_str
            })
            
        logger.info(f"ğŸ• ä¿®å¤åçš„æ—¶é—´æˆ³æ ¼å¼ç¤ºä¾‹: {batch_updates[0]['telemetry_last_updated'] if batch_updates else 'N/A'}")
        
        # è°ƒç”¨æ‰¹é‡æ›´æ–°API
        url = f"{self.api_base_url}/environments/{environment_id}/twins/batch-update"
        
        for attempt in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
            try:
                async with self.http_session.post(url, json=batch_updates) as response:
                    if response.status == 200:
                        result = await response.json()
                        success_count = result.get("success_count", 0)
                        self.metrics.twins_updated += success_count
                        
                        if success_count != len(batch_updates):
                            failed_count = len(batch_updates) - success_count
                            logger.warning(f"æ‰¹é‡æ›´æ–°éƒ¨åˆ†å¤±è´¥: {failed_count} ä¸ªTwinæ›´æ–°å¤±è´¥")
                            self.metrics.update_failures += failed_count
                        
                        # å‘é€Twinå±æ€§æ›´æ–°é€šçŸ¥
                        await self._send_twin_update_notifications(environment_id, batch)
                        
                        logger.info(f"âœ… æˆåŠŸæ‰¹é‡æ›´æ–° {success_count} ä¸ªTwin")
                        break
                        
                    elif response.status == 429:  # Rate limited
                        wait_time = 2 ** attempt
                        logger.warning(f"APIé™æµï¼Œç­‰å¾… {wait_time}s åé‡è¯•")
                        await asyncio.sleep(wait_time)
                        
                    else:
                        error_text = await response.text()
                        logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status} - {error_text}")
                        if attempt == 2:  # æœ€åä¸€æ¬¡å°è¯•
                            self.metrics.update_failures += len(batch_updates)
                        
            except asyncio.TimeoutError:
                logger.warning(f"APIè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/3)")
                if attempt == 2:
                    self.metrics.update_failures += len(batch_updates)
                    
            except Exception as e:
                logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
                if attempt == 2:
                    self.metrics.update_failures += len(batch_updates)

    async def _send_twin_update_notifications(self, environment_id: str, batch: Dict):
        """å‘é€Twinå±æ€§æ›´æ–°é€šçŸ¥"""
        if not self.notification_service:
            return
        
        try:
            for twin_id, data in batch.items():
                # å‘é€Twinå±æ€§æ›´æ–°é€šçŸ¥
                await self.notification_service.send_twin_properties_updated_notification(
                    environment_id=environment_id,
                    twin_id=twin_id,
                    updated_properties=data["properties"],
                    timestamp=data["timestamp"]
                )
                self.metrics.notifications_sent += 1
                
        except Exception as e:
            logger.error(f"å‘é€Twinæ›´æ–°é€šçŸ¥å¤±è´¥: {e}")

    async def monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            await asyncio.sleep(30)
            self.metrics.log_status()
            
            # è¾“å‡ºé˜Ÿåˆ—çŠ¶æ€
            for env_id, queue in self.update_queues.items():
                if queue.qsize() > 0:
                    logger.info(f"ç¯å¢ƒ {env_id} é˜Ÿåˆ—å¤§å°: {queue.qsize()}")

    async def status_broadcaster_loop(self):
        """çŠ¶æ€å¹¿æ’­å¾ªç¯ - å®šæœŸå‘æ‰€æœ‰ç¯å¢ƒå‘é€ç³»ç»ŸçŠ¶æ€"""
        while self.running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿå¹¿æ’­ä¸€æ¬¡çŠ¶æ€
                
                # æ„å»ºç³»ç»ŸçŠ¶æ€
                status = {
                    "service": "twin_updater",
                    "status": "running",
                    "uptime_seconds": int((datetime.now() - self.metrics.start_time).total_seconds()),
                    "metrics": {
                        "messages_consumed": self.metrics.messages_consumed,
                        "twins_updated": self.metrics.twins_updated,
                        "update_failures": self.metrics.update_failures,
                        "notifications_sent": self.metrics.notifications_sent
                    },
                    "queue_sizes": {
                        env_id: queue.qsize() 
                        for env_id, queue in self.update_queues.items()
                    }
                }
                
                # å‘æ‰€æœ‰æ´»è·ƒç¯å¢ƒå¹¿æ’­çŠ¶æ€
                for env_id in self.update_queues.keys():
                    if self.notification_service:
                        await self.notification_service.send_system_status_notification(env_id, status)
                
            except Exception as e:
                logger.error(f"çŠ¶æ€å¹¿æ’­å¤±è´¥: {e}")

    async def stop(self):
        """ä¼˜é›…åœæ­¢"""
        logger.info("æ­£åœ¨åœæ­¢Twin Updater Service...")
        self.running = False
        
        # ç­‰å¾…æ‰€æœ‰é˜Ÿåˆ—æ¸…ç©º
        for env_id, queue in self.update_queues.items():
            wait_time = 0
            while not queue.empty() and wait_time < 30:
                logger.info(f"ç­‰å¾…ç¯å¢ƒ {env_id} é˜Ÿåˆ—æ¸…ç©º: {queue.qsize()}")
                await asyncio.sleep(1)
                wait_time += 1
        
        # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
        for task in self._tasks:
            if not task.done():
                task.cancel()
        
        await asyncio.gather(*self._tasks, return_exceptions=True)
        
        # å…³é—­è¿æ¥
        if self.http_session:
            await self.http_session.close()
        if self.redis_client:
            await self.redis_client.close()
        if self.consumer:
            await self.consumer.stop()
        if self.db_client:
            self.db_client.close()
            
        self.metrics.log_status()
        logger.info("æœåŠ¡å·²å®Œå…¨åœæ­¢")

# WebSocketé€šçŸ¥ç›‘å¬æœåŠ¡ï¼ˆåœ¨FastAPIåº”ç”¨ä¸­è¿è¡Œï¼‰
class WebSocketNotificationListener:
    """WebSocketé€šçŸ¥ç›‘å¬æœåŠ¡ - ç›‘å¬Redisé¢‘é“å¹¶è½¬å‘ç»™WebSocketå®¢æˆ·ç«¯"""
    
    def __init__(self, redis_client, websocket_manager):
        self.redis_client = redis_client
        self.websocket_manager = websocket_manager
        self.running = False
        self.subscription_task = None
    
    async def start(self):
        """å¯åŠ¨ç›‘å¬æœåŠ¡"""
        self.running = True
        self.subscription_task = asyncio.create_task(self._listen_for_notifications())
        logger.info("WebSocketé€šçŸ¥ç›‘å¬æœåŠ¡å·²å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢ç›‘å¬æœåŠ¡"""
        self.running = False
        if self.subscription_task:
            self.subscription_task.cancel()
        logger.info("WebSocketé€šçŸ¥ç›‘å¬æœåŠ¡å·²åœæ­¢")
    
    async def _listen_for_notifications(self):
        """ç›‘å¬Redisé€šçŸ¥é¢‘é“"""
        pubsub = self.redis_client.pubsub()
        
        try:
            # è®¢é˜…æ‰€æœ‰WebSocketé€šçŸ¥é¢‘é“
            await pubsub.psubscribe("ws_notifications:*")
            
            while self.running:
                try:
                    message = await pubsub.get_message(timeout=1.0)
                    if message and message['type'] == 'pmessage':
                        await self._handle_notification(message)
                        
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    logger.error(f"å¤„ç†é€šçŸ¥æ¶ˆæ¯å¤±è´¥: {e}")
                    
        except Exception as e:
            logger.error(f"Redisè®¢é˜…å¤±è´¥: {e}")
        finally:
            await pubsub.unsubscribe()
    
    async def _handle_notification(self, message):
        """å¤„ç†é€šçŸ¥æ¶ˆæ¯"""
        try:
            # è§£æé¢‘é“åè·å–environment_id
            channel = message['channel'].decode('utf-8')
            environment_id = channel.split(':')[1]
            
            # è§£ææ¶ˆæ¯å†…å®¹
            notification_data = json.loads(message['data'].decode('utf-8'))
            
            # è½¬å‘ç»™å¯¹åº”ç¯å¢ƒçš„WebSocketè¿æ¥
            await self.websocket_manager.broadcast_to_environment(
                notification_data, environment_id
            )
            
            logger.debug(f"è½¬å‘é€šçŸ¥åˆ°ç¯å¢ƒ {environment_id}: {notification_data['type']}")
            
        except Exception as e:
            logger.error(f"å¤„ç†WebSocketé€šçŸ¥å¤±è´¥: {e}")

# ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•° - å¯åŠ¨Twin Updater Service"""
    service = OptimizedTwinUpdaterService()
    
    try:
        await service.start()
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
    except Exception as e:
        logger.error(f"æœåŠ¡è¿è¡Œé”™è¯¯: {e}")
    finally:
        await service.stop()

if __name__ == "__main__":
    # è¿è¡Œä¸»æœåŠ¡
    asyncio.run(main())